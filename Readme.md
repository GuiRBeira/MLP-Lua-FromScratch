# MLP-Lua-FromScratch (BossaNet)

Mini-framework de MLP constru√≠do do zero em Lua. Inclui backpropagation manual, otimizadores (SGD/Adam), fun√ß√µes de ativa√ß√£o selecion√°veis e visualiza√ß√£o com Gnuplot.

![Fronteira de Decis√£o do XOR](grafico_fronteira_decisao_adam.png)

## üìñ Sobre o Projeto

Este projeto nasceu a partir de um exerc√≠cio acad√™mico para a disciplina de Sistemas Inteligentes 2, que propunha a implementa√ß√£o de uma Rede Neural (MLP) para resolver o problema do XOR. O desafio foi levado um passo adiante: em vez de usar bibliotecas prontas, todo o sistema foi constru√≠do do zero (from scratch) em Lua, para aprofundar o entendimento sobre os fundamentos matem√°ticos e algor√≠tmicos do Deep Learning.

O que come√ßou como um simples script evoluiu para um mini-framework modular, flex√≠vel e experimental, demonstrando n√£o apenas a solu√ß√£o do problema, mas tamb√©m o processo realista de engenharia, depura√ß√£o e otimiza√ß√£o de um modelo de Machine Learning.

## ‚ú® Features Principais

* **Constru√ß√£o do Zero (From Scratch):** Nenhuma biblioteca de Machine Learning foi utilizada.
    * **`matlib.lua`**: Uma biblioteca de √°lgebra linear para opera√ß√µes com matrizes e vetores, criada especificamente para este projeto.
    * **`neural_net.lua`**: Um motor de treinamento que implementa manualmente o backpropagation.
    * **`plotlib.lua`**: Uma biblioteca de visualiza√ß√£o que gera scripts e renderiza gr√°ficos atrav√©s de uma interface com o Gnuplot.
* **Motor de Rede Neural Modular:** O c√≥digo √© organizado de forma que o motor de treinamento seja agn√≥stico √† visualiza√ß√£o e √† configura√ß√£o do experimento.
* **Otimizadores Selecion√°veis:**
Suas op√ß√µes s√£o:
    * Gradiente Descendente Estoc√°stico (SGD)
    * Adam
* **Fun√ß√µes de Ativa√ß√£o Flex√≠veis:** Suporte para m√∫ltiplas fun√ß√µes de ativa√ß√£o, tamb√©m selecion√°veis por par√¢metro:
    * Sigmoide
    * Tangente Hiperb√≥lica (C√∫bica Suavizada)
    * Fun√ß√µes alternativas ("cubic" = x¬≥+0.5, fun√ß√£o utilizada no exec√≠cio original)
* **T√©cnicas Avan√ßadas de Treinamento:**
    * **Parada Antecipada (Early Stopping):** O treinamento √© interrompido automaticamente quando a acur√°cia de 100% √© atingida.
    * **Corte de Gradiente (Gradient Clipping):** Implementado para previnir o problema de explos√£o de gradientes.
* **Interface de Treinamento Profissional:** A sa√≠da do terminal exibe uma barra de progresso, perda (loss) e acur√°cia a cada √©poca, simulando o comportamento de frameworks como Keras/TensorFlow.

## üöÄ Como Executar

### Pr√©-requisitos

Voc√™ precisar√° ter o **Lua** e o **Gnuplot** instalados e acess√≠veis no seu sistema.

```bash
# Para sistemas baseados em Debian/Ubuntu
sudo apt-get update
sudo apt-get install lua5.4 gnuplot